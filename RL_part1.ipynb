{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we define the transition probability for each action at each state.\n",
    "\n",
    "T = np.array([\n",
    "    [[0.7, 0.3, 0], [1, 0,0], [0.8, 0.2, 0]],\n",
    "    [[0, 1, 0], [nan, nan, nan], [0, 0, 1]],\n",
    "    [[nan, nan, nan], [0.8, 0.1, 0.1], [nan, nan, nan]],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we define the reward while performing an each action at each state.\n",
    "\n",
    "R = np.array([\n",
    "    [[10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "    [[0, 0, 0], [nan, nan, nan], [0, 0, -50]],\n",
    "    [[nan, nan, nan], [40, 0, 0], [nan, nan, nan]],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#possible action that we can perform at each state \n",
    "possible_actions = [[0,1,2],[0,2], [1]]\n",
    "#disount factor\n",
    "lr = 0.95\n",
    "#number of iteration\n",
    "n_iterate = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to define V-value of each state.\n",
    "'''We start by initialize the value funtion at each state with impossible'''\n",
    "def V_values(lr, n_iterate, T, R, possible_actions):\n",
    "    '''\n",
    "    Inputs: lr dicount factor\n",
    "            n_iterate number of iteration\n",
    "            T table of transition probavility\n",
    "            R table of reward\n",
    "            possible_actions different actions that can be take at each state.\n",
    "    Return V vector of Q values\n",
    "    '''\n",
    "    V = np.full(3, -np.inf)\n",
    "    for state, actions in enumerate(possible_actions):\n",
    "        V[state] = 0.0\n",
    "    for i in range(n_iterate):\n",
    "        V_prev = V.copy()\n",
    "        for s in range(3):\n",
    "            V[s] = np.max([np.sum([\n",
    "                T[s,a,sp]*(R[s,a,sp] + lr*(V_prev[sp])) for sp in range(3)\n",
    "            ]) for a in possible_actions[s]\n",
    "                          ])\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.max([np.sum([] for sp in range(3))] for a in possible_actions[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21.89925005,  1.17982024, 53.87349498])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_values(lr,n_iterate, T, R, possible_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_values(lr, n_iterate, T, R, possible_actions):\n",
    "    '''\n",
    "    Inputs: lr dicount factor\n",
    "            n_iterate number of iteration\n",
    "            T table of transition probavility\n",
    "            R table of reward\n",
    "            possible_actions different actions that can be take at each state.\n",
    "    Return Q matrix of Q values\n",
    "    '''\n",
    "    Q = np.full((3,3), -np.inf)\n",
    "    for state, actions in enumerate(possible_actions):\n",
    "        Q[state, actions] = 0.0\n",
    "    for i in range(n_iterate):\n",
    "        Q_prev = Q.copy()\n",
    "        for s in range(3):\n",
    "            for a in possible_actions[s]:\n",
    "                Q[s,a] = np.sum([\n",
    "                    T[s,a,sp]*(R[s,a,sp] + lr*np.max(Q_prev[sp])) for sp in range(3)\n",
    "                ])\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By runing the Q-values iterative algorithm, we find the following result:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21.89925005, 20.80428755, 16.86759588],\n",
       "       [ 1.12082922,        -inf,  1.17982024],\n",
       "       [       -inf, 53.87349498,        -inf]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values(lr,n_iterate, T, R, possible_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This algorithm is verry necessary in reinforcement learning because it gives the value at each state while perfoming a specific action. Since the goal here is to find the optimal policy that can maximize the cumulative rewards, by taking the argmax() function ones will best actions to take.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best action to take at each state is [0 2 1]\n"
     ]
    }
   ],
   "source": [
    "best_act = np.argmax(Q, axis = 1)\n",
    "print('The best action to take at each state is {}.'.format(best_act))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Temporal difference learning algorithm**\n",
    "RL problem with discret actions can be modeled using MDP. in TD, the agent has partial information about the MDP. In generale, we assume that the agent initially know the states and actions to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rate0= 0.05\n",
    "lr_rate_decay=0.1\n",
    "gamma= 0.95\n",
    "n_iterates = 10000000\n",
    "\n",
    "def V_value_TD(lr_rate0, lr_rate_decay, gamma, T, R, n_iterates):\n",
    "    V = np.full(3, -np.inf)\n",
    "    for state, actions in enumerate(possible_actions):\n",
    "        V[state] = 0.0\n",
    "    s = 0\n",
    "    for iterate in range(n_iterates):\n",
    "        V_prev = V.copy()\n",
    "        lr_rate = lr_rate0 / (1 + iterate*lr_rate_decay)\n",
    "        a = np.random.choice(possible_actions[s])\n",
    "        sp = np.random.choice(range(3), p = T[s,a])\n",
    "        r = R[s,a,sp]\n",
    "        V[s] = (1 - lr_rate)*V_prev[s] + lr_rate*(r + gamma*V_prev[sp])\n",
    "        s = sp\n",
    "    return V\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -0.14950054, -20.81673127,  19.51785965])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_value_TD(lr_rate0, lr_rate_decay, gamma, T, R, n_iterates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_value_TD(lr_rate0, lr_rate_decay, gamma, T, R, n_iterates):\n",
    "    Q = np.full((3,3), -np.inf)\n",
    "    s = 0\n",
    "    for state, actions in enumerate(possible_actions):\n",
    "        Q[state, actions] = 0\n",
    "    for iterate in range(n_iterates):\n",
    "        Q_prev = Q.copy()\n",
    "        lr_rate = lr_rate0 / (1 + iterate*lr_rate_decay)\n",
    "        a = np.random.choice(possible_actions[s])\n",
    "        sp = np.random.choice(range(3), p = T[s,a])\n",
    "        r = R[s,a,sp]\n",
    "        Q[s,a] = (1 - lr_rate)*Q_prev[s,a] + lr_rate*(r + gamma*np.max(Q_prev[sp]))\n",
    "        s = sp\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.96310973,   3.73589636,   2.99749087],\n",
       "       [  0.        ,         -inf, -21.04476803],\n",
       "       [        -inf,  22.15838247,         -inf]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_value = Q_value_TD(lr_rate0, lr_rate_decay, gamma, T, R, n_iterates)\n",
    "Q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Optimal policy\n",
    "np.argmax(Q_value, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install gym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation of Q-learning using gym enveronment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation spaces: Discrete(16)\n",
      "Actions spaces: Discrete(4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gym.spaces.discrete.Discrete"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from gym.envs.registration import register\n",
    "from IPython.display import clear_output\n",
    "try:\n",
    "    register(\n",
    "        id='FrozenLakeNolip-v0',\n",
    "        entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "        kwargs={'map_name' : '4x4', 'is_slippery':False},\n",
    "        max_episode_steps=100,\n",
    "        reward_threshold=0.78, # optimum = .8196\n",
    "    )\n",
    "except:\n",
    "    pass\n",
    "\n",
    "env_names = 'FrozenLakeNolip-v0'\n",
    "env = gym.make(env_names)\n",
    "print('Observation spaces:', env.observation_space)\n",
    "print('Actions spaces:', env.action_space)\n",
    "type(env.action_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qagent():\n",
    "    def __init__(self, env):\n",
    "        self.action_size = env.action_space.n\n",
    "\n",
    "    def get_action(self, state):\n",
    "        action = np.random.choice(range(self.action_size))\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Qagent):\n",
    "    def __init__(self, env, discount_rate= 0.95, learning_rate=0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.eps = 1.0\n",
    "        self.model()\n",
    "        \n",
    "    def model(self):\n",
    "        self.q_value = 1e-5 * np.random.random((self.state_size, self.action_size))\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        #we first get the row containing the q_value for the given state.\n",
    "        q_state = self.q_value[state]\n",
    "        #exploitation\n",
    "        action_greedy = np.argmax(q_state)\n",
    "        #exploiration \n",
    "        action_random = super().get_action(state)\n",
    "        \n",
    "        return action_random if np.random.random() < self.eps else action_greedy\n",
    "    \n",
    "    \n",
    "    def train(self, lis):\n",
    "        state, action, next_state, reward, done= lis\n",
    "        q_next =    self.q_value[next_state]\n",
    "        q_next =    np.zeros(self.action_size) if done else q_next\n",
    "        q_target =  reward + self.discount_rate * np.max(q_next)\n",
    "        q_update =  q_target - self.q_value[state,action]\n",
    "        self.q_value[state,action]  +=  self.learning_rate * q_update\n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            self.eps = self.eps * 0.99\n",
    "            \n",
    "agent = Agent(env)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s: 15 a: 2 reward 1.0\n",
      "Episode 99, Total rewards 13.0, eps 0.36603234127322926\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "[[5.97170587e-06 7.20643562e-06 5.20966258e-06 2.85651206e-06]\n",
      " [4.25270648e-06 5.93326848e-06 6.35704204e-06 4.93587023e-06]\n",
      " [1.78204483e-06 4.96594090e-06 3.86067345e-06 5.47007061e-06]\n",
      " [1.79761676e-06 6.11245666e-06 4.99850984e-06 3.66892095e-06]\n",
      " [2.65504517e-06 7.06235342e-06 2.61266295e-06 7.54317372e-06]\n",
      " [9.79798540e-06 3.40974522e-07 6.76267624e-06 6.96989198e-06]\n",
      " [5.94996322e-06 6.40421567e-05 7.67579135e-06 8.02206002e-06]\n",
      " [1.95227846e-06 7.42049414e-06 6.64414663e-06 9.53822404e-06]\n",
      " [5.97750779e-06 6.85830100e-06 1.28469866e-05 3.97660882e-06]\n",
      " [3.57804951e-06 6.00256655e-06 2.24217352e-04 9.39374950e-07]\n",
      " [5.36911201e-06 6.37624983e-03 1.03903169e-06 2.00314731e-06]\n",
      " [6.21341162e-06 3.23348402e-06 2.76927891e-06 3.70475141e-06]\n",
      " [7.99076382e-06 1.87016958e-06 4.99258549e-06 5.43836616e-07]\n",
      " [8.21809378e-06 1.52950597e-05 2.36137633e-03 2.54532904e-07]\n",
      " [2.48140600e-05 1.08052199e-03 1.22479586e-01 1.04658557e-05]\n",
      " [8.01227234e-06 5.90939542e-06 6.73019667e-06 8.57596189e-06]]\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "for ep in range(100):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        agent.train((state, action, next_state, reward, done))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        print('s:', state, 'a:',action, 'reward', reward)\n",
    "        print('Episode {}, Total rewards {}, eps {}'.format(ep,total_reward, agent.eps))\n",
    "        env.render()\n",
    "        print(agent.q_value)\n",
    "        time.sleep(0.02)\n",
    "        clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
